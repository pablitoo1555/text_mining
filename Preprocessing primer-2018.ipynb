{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's talk about preprocessing text\n",
    "\n",
    "#### Reminder:  Goal is to transform text into numeric information that can be used for classification or preditiction.\n",
    "\n",
    "#### Recall:\n",
    "* Vectorizers create feature spaces which have column representations for unique tokens and row representations for documents in a corpus (or collection)\n",
    "* There are several ways to affect your feature space by pre-processing the text.\n",
    "* Some options are available as part of the vectorizer method that creates the feature space, others need to be done before calling the vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things that need to be done PRIOR to creating the feature space.\n",
    "* Anything that will SUBSTITUTE one token for another:  dictionary replacement\n",
    "* Anything that affects the options for the vectorizer: custom stopword lists\n",
    "* You may choose to do stemming and lemmitization first\n",
    "\n",
    "### Things that are options as part of the vector space creation:\n",
    "* Removal of stopwords \n",
    "* Change to lower case\n",
    "* You can embed stemmers to your vectorizer call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 17\n",
      "<class 'str'>\n",
      "[\"It is also the story of a book, a book called The Hitch Hiker's Guide to the Galaxy - not an Earth book, never published on Earth, and until the terrible catastrophe occurred, never seen or heard of by any Earthman.\"]\n"
     ]
    }
   ],
   "source": [
    "text = [\"Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun.\",\n",
    "\"Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape-descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea.\",\n",
    "\"This planet has - or rather had - a problem, which was this: most of the people on it were unhappy for pretty much of the time. Many solutions were suggested for this problem, but most of these were largely concerned with the movements of small green pieces of paper, which is odd because on the whole it wasn't the small green pieces of paper that were unhappy.\",\n",
    "\"And so the problem remained; lots of the people were mean, and most of them were miserable, even the ones with digital watches.\",\n",
    "\"Many were increasingly of the opinion that they'd all made a big mistake in coming down from the trees in the first place. And some said that even the trees had been a bad move, and that no one should ever have left the oceans.\",\n",
    "\"And then, one Thursday, nearly two thousand years after one man had been nailed to a tree for saying how great it would be to be nice to people for a change, one girl sitting on her own in a small cafe in Rickmansworth suddenly realized what it was that had been going wrong all this time, and she finally knew how the world could be made a good and happy place. This time it was right, it would work, and no one would have to get nailed to anything.\",\n",
    "\"Sadly, however, before she could get to a phone to tell anyone about it, a terribly stupid catastrophe occurred, and the idea was lost forever.\",\n",
    "\"This is not her story.\",\n",
    "\"But it is the story of that terrible stupid catastrophe and some of its consequences.\",\n",
    "\"It is also the story of a book, a book called The Hitch Hiker's Guide to the Galaxy - not an Earth book, never published on Earth, and until the terrible catastrophe occurred, never seen or heard of by any Earthman.\",\n",
    "\"Nevertheless, a wholly remarkable book.\",\n",
    "\"In fact it was probably the most remarkable book ever to come out of the great publishing houses of Ursa Minor - of which no Earthman had ever heard either.\",\n",
    "\"Not only is it a wholly remarkable book, it is also a highly successful one - more popular than the Celestial Home Care Omnibus, better selling than Fifty More Things to do in Zero Gravity, and more controversial than Oolon Colluphid's trilogy of philosophical blockbusters Where God Went Wrong, Some More of God's Greatest Mistakes and Who is this God Person Anyway?\",\n",
    "\"In many of the more relaxed civilizations on the Outer Eastern Rim of the Galaxy, the Hitch Hiker's Guide has already supplanted the great Encyclopedia Galactica as the standard repository of all knowledge and wisdom, for though it has many omissions and contains much that is apocryphal, or at least wildly inaccurate, it scores over the older, more pedestrian work in two important respects.\",\n",
    "\"First, it is slightly cheaper; and secondly it has the words Don't Panic inscribed in large friendly letters on its cover.\",\n",
    "\"But the story of this terrible, stupid Thursday, the story of its extraordinary consequences, and the story of how these consequences are inextricably intertwined with this remarkable book begins very simply.\",\n",
    "\"It begins with a house. \"]\n",
    "print(type(text), len(text))\n",
    "print(type(text[9]))\n",
    "print(text[9:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tokens in a list is an alternative way of processing text.  You can leave it as strings.  \n",
    "### The difference is that lists have individual items already defined to process individually; strings need to be broken into those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 47\n",
      "['It', 'is', 'also', 'the', 'story', 'of', 'a', 'book', ',', 'a']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(text[9]) #creates a list of tokens from a string that is in the 0 element of the list \"text\"\n",
    "print(type(tokens), len(tokens))\n",
    "print(tokens[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 287)\n",
      "['And', 'Anyway', 'But', 'Care', 'Celestial', 'Colluphid', 'Don', 'Earth', 'Earthman', 'Eastern', 'Encyclopedia', 'Far', 'Fifty', 'First', 'Galactica', 'Galaxy', 'God', 'Gravity', 'Greatest', 'Guide', 'Hiker', 'Hitch', 'Home', 'In', 'It', 'Many', 'Minor', 'Mistakes', 'More', 'Nevertheless', 'Not', 'Omnibus', 'Oolon', 'Orbiting', 'Outer', 'Panic', 'Person', 'Rickmansworth', 'Rim', 'Sadly', 'Some', 'The', 'Things', 'This', 'Thursday', 'Ursa', 'Went', 'Where', 'Who', 'Wrong', 'Zero', 'about', 'after', 'all', 'already', 'also', 'amazingly', 'an', 'and', 'any', 'anyone', 'anything', 'ape', 'apocryphal', 'are', 'arm', 'as', 'at', 'backwaters', 'bad', 'be', 'because', 'been', 'before', 'begins', 'better', 'big', 'blockbusters', 'blue', 'book', 'but', 'by', 'cafe', 'called', 'catastrophe', 'change', 'cheaper', 'civilizations', 'come', 'coming', 'concerned', 'consequences', 'contains', 'controversial', 'could', 'cover', 'descended', 'digital', 'distance', 'do', 'down', 'either', 'end', 'even', 'ever', 'extraordinary', 'fact', 'finally', 'first', 'for', 'forever', 'forms', 'friendly', 'from', 'get', 'girl', 'going', 'good', 'great', 'green', 'had', 'happy', 'has', 'have', 'heard', 'her', 'highly', 'house', 'houses', 'how', 'however', 'idea', 'important', 'in', 'inaccurate', 'increasingly', 'inextricably', 'inscribed', 'insignificant', 'intertwined', 'is', 'it', 'its', 'knew', 'knowledge', 'large', 'largely', 'least', 'left', 'letters', 'lies', 'life', 'little', 'lost', 'lots', 'made', 'man', 'many', 'mean', 'miles', 'million', 'miserable', 'mistake', 'more', 'most', 'move', 'movements', 'much', 'nailed', 'nearly', 'neat', 'never', 'nice', 'ninety', 'no', 'not', 'occurred', 'oceans', 'odd', 'of', 'older', 'omissions', 'on', 'one', 'ones', 'only', 'opinion', 'or', 'out', 'over', 'own', 'paper', 'pedestrian', 'people', 'philosophical', 'phone', 'pieces', 'place', 'planet', 'popular', 'pretty', 'primitive', 'probably', 'problem', 'published', 'publishing', 'rather', 'realized', 'relaxed', 'remained', 'remarkable', 'repository', 'respects', 'right', 'roughly', 'said', 'saying', 'scores', 'secondly', 'seen', 'selling', 'she', 'should', 'simply', 'sitting', 'slightly', 'small', 'so', 'solutions', 'some', 'spiral', 'standard', 'still', 'story', 'stupid', 'successful', 'suddenly', 'suggested', 'sun', 'supplanted', 'tell', 'terrible', 'terribly', 'than', 'that', 'the', 'them', 'then', 'these', 'they', 'think', 'this', 'though', 'thousand', 'time', 'to', 'tree', 'trees', 'trilogy', 'two', 'uncharted', 'unfashionable', 'unhappy', 'unregarded', 'until', 'utterly', 'very', 'was', 'wasn', 'watches', 'were', 'western', 'what', 'which', 'whole', 'wholly', 'whose', 'wildly', 'wisdom', 'with', 'words', 'work', 'world', 'would', 'wrong', 'years', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "# preliminary investigation of feature space - you can (should?) see what your data look like before \n",
    "# deciding on what preprocessing you want to do\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "prelim = CountVectorizer(binary=False, lowercase = False)\n",
    "text_dm = prelim.fit_transform(text)\n",
    "print(text_dm.shape)\n",
    "print(prelim.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 17\n",
      "['It', 'is', 'also', 'the', 'stori', 'of', 'a', 'book,', 'a', 'book', 'call', 'the', 'hitch', \"hiker'\", 'guid', 'to', 'the', 'galaxi', '-', 'not', 'an', 'earth', 'book,', 'never', 'publish', 'on', 'earth,', 'and', 'until', 'the', 'terribl', 'catastroph', 'occurred,', 'never', 'seen', 'or', 'heard', 'of', 'by', 'ani', 'earthman.']\n"
     ]
    }
   ],
   "source": [
    "# Let's say for our purposes, it's important to count publish and publishing as the same feature.\n",
    "# So now we do some pre processing\n",
    "\n",
    "#pre processing - stemmers\n",
    "#porter stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# our corpus is a list of strings, we need to stem each word in each string\n",
    "# if this was a dataframe containing rows with strings, we'd need to stem each word in each string in each row\n",
    "# our stemmed text = [ [process each word in the string that gets split into words] process each string in the list]\n",
    "ps_text = [[ps.stem(word) for word in sentence.split()] for sentence in text]\n",
    "print(type(ps_text), len(ps_text))\n",
    "print(ps_text[9])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 17\n",
      "It is also the stori of a book, a book call the hitch hiker' guid to the galaxi - not an earth book, never publish on earth, and until the terribl catastroph occurred, never seen or heard of by ani earthman.\n"
     ]
    }
   ],
   "source": [
    "# we can process the list of lists or join the pieces back into a string\n",
    "ps_text = [\" \".join([ps.stem(word) for word in sentence.split()]) for sentence in text]\n",
    "print(type(ps_text), len(ps_text))\n",
    "print(ps_text[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 276)\n",
      "['In', 'It', 'about', 'after', 'all', 'alreadi', 'also', 'amazingli', 'an', 'and', 'ani', 'anyon', 'anything', 'anyway', 'ape', 'apocryphal', 'are', 'arm', 'as', 'at', 'backwat', 'bad', 'be', 'becaus', 'been', 'befor', 'begin', 'better', 'big', 'blockbust', 'blue', 'book', 'but', 'by', 'cafe', 'call', 'care', 'catastroph', 'celesti', 'change', 'cheaper', 'civil', 'colluphid', 'come', 'concern', 'consequ', 'consequences', 'contain', 'controversi', 'could', 'cover', 'descend', 'digit', 'distanc', 'do', 'don', 'down', 'earth', 'earthman', 'eastern', 'either', 'encyclopedia', 'end', 'even', 'ever', 'extraordinari', 'fact', 'far', 'fifti', 'final', 'first', 'for', 'forever', 'form', 'friendli', 'from', 'galactica', 'galaxi', 'galaxy', 'get', 'girl', 'go', 'god', 'good', 'gravity', 'great', 'greatest', 'green', 'guid', 'ha', 'had', 'happi', 'have', 'heard', 'her', 'highli', 'hiker', 'hitch', 'home', 'hous', 'house', 'how', 'however', 'idea', 'import', 'in', 'inaccurate', 'increasingli', 'inextric', 'inscrib', 'insignific', 'intertwin', 'is', 'it', 'knew', 'knowledg', 'larg', 'least', 'left', 'letter', 'lie', 'life', 'littl', 'lost', 'lot', 'made', 'man', 'mani', 'mean', 'mile', 'million', 'minor', 'miserable', 'mistak', 'more', 'most', 'move', 'movement', 'much', 'nail', 'nearli', 'neat', 'never', 'nevertheless', 'nice', 'ninety', 'no', 'not', 'occurred', 'oceans', 'odd', 'of', 'older', 'omiss', 'omnibus', 'on', 'one', 'onli', 'oolon', 'opinion', 'or', 'orbit', 'out', 'outer', 'over', 'own', 'panic', 'paper', 'pedestrian', 'peopl', 'person', 'philosoph', 'phone', 'piec', 'place', 'planet', 'popular', 'pretti', 'primit', 'probabl', 'problem', 'publish', 'rather', 'realiz', 'relax', 'remained', 'remark', 'repositori', 'respects', 'rickmansworth', 'right', 'rim', 'roughli', 'sadly', 'said', 'say', 'score', 'secondli', 'seen', 'sell', 'she', 'should', 'simply', 'sit', 'slightli', 'small', 'so', 'solut', 'some', 'spiral', 'standard', 'still', 'stori', 'story', 'stupid', 'success', 'suddenli', 'suggest', 'sun', 'supplant', 'tell', 'terribl', 'terrible', 'than', 'that', 'the', 'them', 'then', 'these', 'they', 'thi', 'thing', 'think', 'this', 'though', 'thousand', 'thursday', 'time', 'to', 'tree', 'trilog', 'two', 'unchart', 'unfashion', 'unhappi', 'unhappy', 'unregard', 'until', 'ursa', 'utterli', 'veri', 'wa', 'wasn', 'watch', 'watches', 'went', 'were', 'western', 'what', 'where', 'which', 'who', 'whole', 'wholli', 'whose', 'wildli', 'wisdom', 'with', 'word', 'work', 'world', 'would', 'wrong', 'year', 'yellow', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# did this change our feature space the way we had hoped?\n",
    "prelim = CountVectorizer(binary=False, lowercase = False)\n",
    "ptext_dm = prelim.fit_transform(ps_text)\n",
    "print(ptext_dm.shape)\n",
    "print(prelim.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 195)\n",
      "['alreadi', 'amazingli', 'ani', 'anyon', 'ape', 'apocryphal', 'arm', 'backwat', 'bad', 'becaus', 'befor', 'begin', 'better', 'big', 'blockbust', 'blue', 'book', 'cafe', 'care', 'catastroph', 'celesti', 'change', 'cheaper', 'civil', 'colluphid', 'come', 'concern', 'consequ', 'consequences', 'contain', 'controversi', 'cover', 'descend', 'digit', 'distanc', 'don', 'earth', 'earthman', 'eastern', 'encyclopedia', 'end', 'extraordinari', 'fact', 'far', 'fifti', 'final', 'forever', 'form', 'friendli', 'galactica', 'galaxi', 'galaxy', 'girl', 'god', 'good', 'gravity', 'great', 'greatest', 'green', 'guid', 'ha', 'happi', 'heard', 'highli', 'hiker', 'hitch', 'home', 'hous', 'house', 'idea', 'import', 'inaccurate', 'increasingli', 'inextric', 'inscrib', 'insignific', 'intertwin', 'knew', 'knowledg', 'larg', 'left', 'letter', 'lie', 'life', 'littl', 'lost', 'lot', 'man', 'mani', 'mean', 'mile', 'million', 'minor', 'miserable', 'mistak', 'movement', 'nail', 'nearli', 'neat', 'nice', 'ninety', 'occurred', 'oceans', 'odd', 'older', 'omiss', 'omnibus', 'onli', 'oolon', 'opinion', 'orbit', 'outer', 'panic', 'paper', 'pedestrian', 'peopl', 'person', 'philosoph', 'phone', 'piec', 'place', 'planet', 'popular', 'pretti', 'primit', 'probabl', 'problem', 'publish', 'realiz', 'relax', 'remained', 'remark', 'repositori', 'respects', 'rickmansworth', 'right', 'rim', 'roughli', 'sadly', 'said', 'say', 'score', 'secondli', 'seen', 'sell', 'simply', 'sit', 'slightli', 'small', 'solut', 'spiral', 'standard', 'stori', 'story', 'stupid', 'success', 'suddenli', 'suggest', 'sun', 'supplant', 'tell', 'terribl', 'terrible', 'thi', 'thing', 'think', 'thousand', 'thursday', 'time', 'tree', 'trilog', 'unchart', 'unfashion', 'unhappi', 'unhappy', 'unregard', 'ursa', 'utterli', 'veri', 'wa', 'wasn', 'watch', 'watches', 'went', 'western', 'wholli', 'wildli', 'wisdom', 'word', 'work', 'world', 'wrong', 'year', 'yellow', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# now let's actually create the feature space of interest\n",
    "CV = CountVectorizer(binary=False, stop_words = \"english\")\n",
    "cvtext_dm = CV.fit_transform(ps_text)\n",
    "print(cvtext_dm.shape)\n",
    "print(CV.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 17\n",
      "It is also the story of a book, a book called The Hitch Hiker's Guide to the Galaxy - not an Earth book, never published on Earth, and until the terrible catastrophe occurred, never seen or heard of by any Earthman.\n"
     ]
    }
   ],
   "source": [
    "# because we were looking at verb tenses, we could have lemmatized instead\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# our corpus is a list of strings, we need to stem each word in each string\n",
    "# if this was a dataframe containing rows with strings, we'd need to process each word in each string in each row\n",
    "# our stemmed text = [put back into a string ([process each word in the string that gets split into words]) process each string in the list]\n",
    "lem_text = [\" \".join([wnl.lemmatize(word) for word in sentence.split()]) for sentence in text]\n",
    "print(type(lem_text), len(lem_text))\n",
    "print(lem_text[9])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 284)\n",
      "['And', 'Anyway', 'But', 'Care', 'Celestial', 'Colluphid', 'Don', 'Earth', 'Earthman', 'Eastern', 'Encyclopedia', 'Far', 'Fifty', 'First', 'Galactica', 'Galaxy', 'God', 'Gravity', 'Greatest', 'Guide', 'Hiker', 'Hitch', 'Home', 'In', 'It', 'Many', 'Minor', 'Mistakes', 'More', 'Nevertheless', 'Not', 'Omnibus', 'Oolon', 'Orbiting', 'Outer', 'Panic', 'Person', 'Rickmansworth', 'Rim', 'Sadly', 'Some', 'The', 'Things', 'This', 'Thursday', 'Ursa', 'Went', 'Where', 'Who', 'Wrong', 'Zero', 'about', 'after', 'all', 'already', 'also', 'amazingly', 'an', 'and', 'any', 'anyone', 'anything', 'ape', 'apocryphal', 'are', 'arm', 'at', 'backwater', 'bad', 'be', 'because', 'been', 'before', 'begin', 'better', 'big', 'blockbuster', 'blue', 'book', 'but', 'by', 'cafe', 'called', 'catastrophe', 'change', 'cheaper', 'civilization', 'come', 'coming', 'concerned', 'consequence', 'consequences', 'contains', 'controversial', 'could', 'cover', 'descended', 'digital', 'distance', 'do', 'down', 'either', 'end', 'even', 'ever', 'extraordinary', 'fact', 'finally', 'first', 'for', 'forever', 'form', 'friendly', 'from', 'get', 'girl', 'going', 'good', 'great', 'green', 'ha', 'had', 'happy', 'have', 'heard', 'her', 'highly', 'house', 'how', 'however', 'idea', 'important', 'in', 'inaccurate', 'increasingly', 'inextricably', 'inscribed', 'insignificant', 'intertwined', 'is', 'it', 'knew', 'knowledge', 'large', 'largely', 'least', 'left', 'letter', 'lie', 'life', 'little', 'lost', 'lot', 'made', 'man', 'many', 'mean', 'mile', 'million', 'miserable', 'mistake', 'more', 'most', 'move', 'movement', 'much', 'nailed', 'nearly', 'neat', 'never', 'nice', 'ninety', 'no', 'not', 'occurred', 'oceans', 'odd', 'of', 'older', 'omission', 'on', 'one', 'only', 'opinion', 'or', 'out', 'over', 'own', 'paper', 'pedestrian', 'people', 'philosophical', 'phone', 'piece', 'place', 'planet', 'popular', 'pretty', 'primitive', 'probably', 'problem', 'published', 'publishing', 'rather', 'realized', 'relaxed', 'remained', 'remarkable', 'repository', 'respects', 'right', 'roughly', 'said', 'saying', 'score', 'secondly', 'seen', 'selling', 'she', 'should', 'simply', 'sitting', 'slightly', 'small', 'so', 'solution', 'some', 'spiral', 'standard', 'still', 'story', 'stupid', 'successful', 'suddenly', 'suggested', 'sun', 'supplanted', 'tell', 'terrible', 'terribly', 'than', 'that', 'the', 'them', 'then', 'these', 'they', 'think', 'this', 'though', 'thousand', 'time', 'to', 'tree', 'trilogy', 'two', 'uncharted', 'unfashionable', 'unhappy', 'unregarded', 'until', 'utterly', 'very', 'wa', 'wasn', 'watch', 'watches', 'were', 'western', 'what', 'which', 'whole', 'wholly', 'whose', 'wildly', 'wisdom', 'with', 'word', 'work', 'world', 'would', 'wrong', 'year', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "# did this change our feature space the way we had hoped?\n",
    "prelim = CountVectorizer(binary=False, lowercase = False)\n",
    "ltext_dm = prelim.fit_transform(lem_text)\n",
    "print(ltext_dm.shape)\n",
    "print(prelim.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 17\n",
      "It be also the story of a book, a book call The Hitch Hiker's Guide to the Galaxy - not an Earth book, never publish on Earth, and until the terrible catastrophe occurred, never see or hear of by any Earthman.\n"
     ]
    }
   ],
   "source": [
    "# no because the default to lemmatizing is to process things as nouns\n",
    "# if we only care about that one verb, set the verb as default\n",
    "\n",
    "lem_text = [\" \".join([wnl.lemmatize(word, pos = \"v\") for word in sentence.split()]) for sentence in text]\n",
    "print(type(lem_text), len(lem_text))\n",
    "print(lem_text[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 276)\n",
      "['And', 'Anyway', 'But', 'Care', 'Celestial', 'Colluphid', 'Don', 'Earth', 'Earthman', 'Eastern', 'Encyclopedia', 'Far', 'Fifty', 'First', 'Galactica', 'Galaxy', 'God', 'Gravity', 'Greatest', 'Guide', 'Hiker', 'Hitch', 'Home', 'In', 'It', 'Many', 'Minor', 'Mistakes', 'More', 'Nevertheless', 'Not', 'Omnibus', 'Oolon', 'Orbiting', 'Outer', 'Panic', 'Person', 'Rickmansworth', 'Rim', 'Sadly', 'Some', 'The', 'Things', 'This', 'Thursday', 'Ursa', 'Went', 'Where', 'Who', 'Wrong', 'Zero', 'about', 'after', 'all', 'already', 'also', 'amazingly', 'an', 'and', 'any', 'anyone', 'anything', 'ape', 'apocryphal', 'arm', 'as', 'at', 'backwaters', 'bad', 'be', 'because', 'before', 'begin', 'better', 'big', 'blockbusters', 'blue', 'book', 'but', 'by', 'cafe', 'call', 'catastrophe', 'change', 'cheaper', 'civilizations', 'come', 'concern', 'consequences', 'contain', 'controversial', 'could', 'cover', 'descended', 'digital', 'distance', 'do', 'down', 'either', 'end', 'even', 'ever', 'extraordinary', 'fact', 'finally', 'first', 'for', 'forever', 'form', 'friendly', 'from', 'get', 'girl', 'go', 'good', 'great', 'green', 'happy', 'have', 'hear', 'her', 'highly', 'house', 'how', 'however', 'idea', 'important', 'in', 'inaccurate', 'increasingly', 'inextricably', 'inscribe', 'insignificant', 'intertwine', 'it', 'its', 'know', 'knowledge', 'large', 'largely', 'least', 'leave', 'letter', 'lie', 'life', 'little', 'lose', 'lot', 'make', 'man', 'many', 'mean', 'miles', 'million', 'miserable', 'mistake', 'more', 'most', 'move', 'movements', 'much', 'nail', 'nearly', 'neat', 'never', 'nice', 'ninety', 'no', 'not', 'occurred', 'oceans', 'odd', 'of', 'older', 'omissions', 'on', 'one', 'ones', 'only', 'opinion', 'or', 'out', 'over', 'own', 'paper', 'pedestrian', 'people', 'philosophical', 'phone', 'piece', 'place', 'planet', 'popular', 'pretty', 'primitive', 'probably', 'problem', 'publish', 'rather', 'realize', 'relax', 'remained', 'remarkable', 'repository', 'respects', 'right', 'roughly', 'say', 'score', 'secondly', 'see', 'sell', 'she', 'should', 'simply', 'sit', 'slightly', 'small', 'so', 'solutions', 'some', 'spiral', 'standard', 'still', 'story', 'stupid', 'successful', 'suddenly', 'suggest', 'sun', 'supplant', 'tell', 'terrible', 'terribly', 'than', 'that', 'the', 'them', 'then', 'these', 'they', 'think', 'this', 'though', 'thousand', 'time', 'to', 'tree', 'trilogy', 'two', 'uncharted', 'unfashionable', 'unhappy', 'unregarded', 'until', 'utterly', 'very', 'wasn', 'watch', 'watches', 'western', 'what', 'which', 'whole', 'wholly', 'whose', 'wildly', 'wisdom', 'with', 'word', 'work', 'world', 'would', 'wrong', 'years', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "# did this change our feature space the way we had hoped?\n",
    "prelim = CountVectorizer(binary=False, lowercase = False)\n",
    "ltext_dm = prelim.fit_transform(lem_text)\n",
    "print (ltext_dm.shape)\n",
    "print(prelim.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 180)\n",
      "['amazingly', 'ape', 'apocryphal', 'arm', 'backwaters', 'bad', 'begin', 'better', 'big', 'blockbusters', 'blue', 'book', 'cafe', 'care', 'catastrophe', 'celestial', 'change', 'cheaper', 'civilizations', 'colluphid', 'come', 'concern', 'consequences', 'contain', 'controversial', 'cover', 'descended', 'digital', 'distance', 'don', 'earth', 'earthman', 'eastern', 'encyclopedia', 'end', 'extraordinary', 'fact', 'far', 'finally', 'forever', 'form', 'friendly', 'galactica', 'galaxy', 'girl', 'god', 'good', 'gravity', 'great', 'greatest', 'green', 'guide', 'happy', 'hear', 'highly', 'hiker', 'hitch', 'home', 'house', 'idea', 'important', 'inaccurate', 'increasingly', 'inextricably', 'inscribe', 'insignificant', 'intertwine', 'know', 'knowledge', 'large', 'largely', 'leave', 'letter', 'lie', 'life', 'little', 'lose', 'lot', 'make', 'man', 'mean', 'miles', 'million', 'minor', 'miserable', 'mistake', 'mistakes', 'movements', 'nail', 'nearly', 'neat', 'nice', 'ninety', 'occurred', 'oceans', 'odd', 'older', 'omissions', 'omnibus', 'ones', 'oolon', 'opinion', 'orbiting', 'outer', 'panic', 'paper', 'pedestrian', 'people', 'person', 'philosophical', 'phone', 'piece', 'place', 'planet', 'popular', 'pretty', 'primitive', 'probably', 'problem', 'publish', 'realize', 'relax', 'remained', 'remarkable', 'repository', 'respects', 'rickmansworth', 'right', 'rim', 'roughly', 'sadly', 'say', 'score', 'secondly', 'sell', 'simply', 'sit', 'slightly', 'small', 'solutions', 'spiral', 'standard', 'story', 'stupid', 'successful', 'suddenly', 'suggest', 'sun', 'supplant', 'tell', 'terrible', 'terribly', 'things', 'think', 'thousand', 'thursday', 'time', 'tree', 'trilogy', 'uncharted', 'unfashionable', 'unhappy', 'unregarded', 'ursa', 'utterly', 'wasn', 'watch', 'watches', 'went', 'western', 'wholly', 'wildly', 'wisdom', 'word', 'work', 'world', 'wrong', 'years', 'yellow', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# now let's actually create the feature space of interest\n",
    "CV = CountVectorizer(binary=False, stop_words = \"english\")\n",
    "cvtext_dm = CV.fit_transform(lem_text)\n",
    "print(cvtext_dm.shape)\n",
    "print(CV.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> It <class 'str'>\n",
      "<class 'list'> ('It', 'PRP') <class 'tuple'>\n",
      "It\n",
      "PRP\n"
     ]
    }
   ],
   "source": [
    "#how to use POS tagging to improve lemmatization\n",
    "import nltk\n",
    "\n",
    "onestring = word_tokenize(text[9])\n",
    "print(type(onestring), onestring[0], type(onestring[0]))\n",
    "taggedlist = nltk.pos_tag(onestring)\n",
    "print(type(taggedlist), taggedlist[0], type(taggedlist[0]))\n",
    "print(taggedlist[0][0])\n",
    "print(taggedlist[0][1])\n",
    "\n",
    "# we know that nouns begin with N, verbs with V, adverbs with R, and adjectives with J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('It', 'PRP') P\n",
      "('is', 'VBZ') V\n",
      "('also', 'RB') R\n",
      "('the', 'DT') D\n",
      "('story', 'NN') N\n",
      "('of', 'IN') I\n",
      "('a', 'DT') D\n",
      "('book', 'NN') N\n",
      "(',', ',') ,\n",
      "('a', 'DT') D\n",
      "('book', 'NN') N\n",
      "('called', 'VBN') V\n",
      "('The', 'DT') D\n",
      "('Hitch', 'NNP') N\n",
      "('Hiker', 'NNP') N\n",
      "(\"'s\", 'POS') P\n",
      "('Guide', 'NNP') N\n",
      "('to', 'TO') T\n",
      "('the', 'DT') D\n",
      "('Galaxy', 'NNP') N\n",
      "('-', ':') :\n",
      "('not', 'RB') R\n",
      "('an', 'DT') D\n",
      "('Earth', 'NN') N\n",
      "('book', 'NN') N\n",
      "(',', ',') ,\n",
      "('never', 'RB') R\n",
      "('published', 'VBN') V\n",
      "('on', 'IN') I\n",
      "('Earth', 'NNP') N\n",
      "(',', ',') ,\n",
      "('and', 'CC') C\n",
      "('until', 'IN') I\n",
      "('the', 'DT') D\n",
      "('terrible', 'JJ') J\n",
      "('catastrophe', 'NN') N\n",
      "('occurred', 'VBD') V\n",
      "(',', ',') ,\n",
      "('never', 'RB') R\n",
      "('seen', 'VBN') V\n",
      "('or', 'CC') C\n",
      "('heard', 'VBN') V\n",
      "('of', 'IN') I\n",
      "('by', 'IN') I\n",
      "('any', 'DT') D\n",
      "('Earthman', 'NNP') N\n",
      "('.', '.') .\n"
     ]
    }
   ],
   "source": [
    "# we know that nouns begin with N, verbs with V, adverbs with R, and adjectives with J\n",
    "for item in taggedlist:\n",
    "    print(item, item[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It be also the story of a book , a book call The Hitch Hiker 's Guide to the Galaxy - not an Earth book , never publish on Earth , and until the terrible catastrophe occur , never see or hear of by any Earthman .\n"
     ]
    }
   ],
   "source": [
    "# way better code for this can be found here: http://www.ling.helsinki.fi/~gwilcock/Tartu-2011/P2-nltk-2.xhtml\n",
    "\n",
    "newlem = []\n",
    "wordtype = set(['R','V','N'])\n",
    "for item in taggedlist:\n",
    "    if item[1][0] in wordtype:\n",
    "        postag = item[1][0].lower()\n",
    "    elif item[1][0] == 'J':\n",
    "        postag = 'a'\n",
    "    else:\n",
    "        postag = \"n\"\n",
    "\n",
    "    lemmed = wnl.lemmatize(item[0], pos = postag)\n",
    "    newlem.append(lemmed)\n",
    "    \n",
    "print(\" \".join(newlem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It be also the story of a book , a book call The Hitch Hiker 's Guide to the Galaxy - not an Earth book , never publish on Earth , and until the terrible catastrophe occur , never see or hear of by any Earthman .\n"
     ]
    }
   ],
   "source": [
    "# put it all together\n",
    "newtext = []\n",
    "wordtype = set(['R','V','N'])\n",
    "\n",
    "for string in text:\n",
    "    newlem = []\n",
    "\n",
    "    taggedlist = nltk.pos_tag(word_tokenize(string))\n",
    "    for item in taggedlist:\n",
    "        if item[1][0] in wordtype:\n",
    "            postag = item[1][0].lower()\n",
    "        elif item[1][0] == 'J':\n",
    "            postag = 'a'\n",
    "        else:\n",
    "            postag = \"n\"\n",
    "\n",
    "        lemmed = wnl.lemmatize(item[0], pos = postag)\n",
    "        newlem.append(lemmed)\n",
    "    \n",
    "    newstring = \" \".join(newlem)\n",
    "    \n",
    "   \n",
    "\n",
    "    newtext =newtext + [newstring]\n",
    "    \n",
    "print(newtext[9])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 176)\n",
      "['amazingly', 'ape', 'apocryphal', 'arm', 'backwater', 'bad', 'begin', 'big', 'blockbuster', 'blue', 'book', 'cafe', 'care', 'catastrophe', 'celestial', 'change', 'cheap', 'civilization', 'colluphid', 'come', 'concern', 'consequence', 'contains', 'controversial', 'cover', 'descended', 'digital', 'distance', 'earth', 'earthman', 'eastern', 'encyclopedia', 'end', 'extraordinary', 'fact', 'far', 'finally', 'forever', 'form', 'friendly', 'galactica', 'galaxy', 'girl', 'god', 'good', 'gravity', 'great', 'greatest', 'green', 'guide', 'happy', 'hear', 'highly', 'hiker', 'hitch', 'home', 'house', 'idea', 'important', 'inaccurate', 'increasingly', 'inextricably', 'inscribe', 'insignificant', 'intertwine', 'know', 'knowledge', 'large', 'largely', 'leave', 'letter', 'lie', 'life', 'little', 'lose', 'lot', 'make', 'man', 'mean', 'mile', 'million', 'minor', 'miserable', 'mistake', 'mistakes', 'movement', 'nail', 'nearly', 'neat', 'nice', 'ninety', 'occur', 'ocean', 'odd', 'old', 'omission', 'omnibus', 'oolon', 'opinion', 'orbiting', 'outer', 'panic', 'paper', 'pedestrian', 'people', 'person', 'philosophical', 'phone', 'piece', 'place', 'planet', 'popular', 'pretty', 'primitive', 'probably', 'problem', 'publish', 'publishing', 'realize', 'relaxed', 'remain', 'remarkable', 'repository', 'respect', 'rickmansworth', 'right', 'rim', 'roughly', 'sadly', 'say', 'score', 'secondly', 'sell', 'simply', 'sit', 'slightly', 'small', 'solution', 'spiral', 'standard', 'story', 'stupid', 'successful', 'suddenly', 'suggest', 'sun', 'supplant', 'tell', 'terrible', 'terribly', 'things', 'think', 'thousand', 'thursday', 'time', 'tree', 'trilogy', 'uncharted', 'unfashionable', 'unhappy', 'unregarded', 'ursa', 'utterly', 'watch', 'went', 'western', 'wholly', 'wildly', 'wisdom', 'word', 'work', 'world', 'wrong', 'year', 'yellow', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# now let's actually create the feature space of interest\n",
    "CV = CountVectorizer(binary=False, stop_words = \"english\")\n",
    "cvtext_dm = CV.fit_transform(newtext)\n",
    "print(cvtext_dm.shape)\n",
    "print(CV.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many were increasingly of the opinion that they'd all made a big mistake in coming down from the trees in the first place. And some said that even the trees had been a bad move, and that no one should ever have left the oceans.\n",
      "~~~~~~~~~~~~~~\n",
      "Not only is it a wholly remarkable book, it is also a highly successful one - more popular than the Celestial Home Care Omnibus, better selling than Fifty More Things to do in Zero Gravity, and more controversial than Oolon Colluphid's trilogy of philosophical blockbusters Where God Went Wrong, Some More of God's Greatest Mistakes and Who is this God Person Anyway?\n"
     ]
    }
   ],
   "source": [
    "# WHY IS MISTAKE/MISTAKES still there?!?\n",
    "# Investigate!\n",
    "\n",
    "print(text[4])\n",
    "print(\"~~~~~~~~~~~~~~\")\n",
    "print(text[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Many', 'JJ'),\n",
       " ('were', 'VBD'),\n",
       " ('increasingly', 'RB'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('opinion', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " (\"'d\", 'MD'),\n",
       " ('all', 'DT'),\n",
       " ('made', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('mistake', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('coming', 'VBG'),\n",
       " ('down', 'RP'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('trees', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('some', 'DT'),\n",
       " ('said', 'VBD'),\n",
       " ('that', 'IN'),\n",
       " ('even', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('trees', 'NNS'),\n",
       " ('had', 'VBD'),\n",
       " ('been', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('bad', 'JJ'),\n",
       " ('move', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('that', 'IN'),\n",
       " ('no', 'DT'),\n",
       " ('one', 'NN'),\n",
       " ('should', 'MD'),\n",
       " ('ever', 'RB'),\n",
       " ('have', 'VB'),\n",
       " ('left', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('oceans', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(text[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Not', 'RB'),\n",
       " ('only', 'RB'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('wholly', 'RB'),\n",
       " ('remarkable', 'JJ'),\n",
       " ('book', 'NN'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('also', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('highly', 'RB'),\n",
       " ('successful', 'JJ'),\n",
       " ('one', 'CD'),\n",
       " ('-', ':'),\n",
       " ('more', 'JJR'),\n",
       " ('popular', 'JJ'),\n",
       " ('than', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Celestial', 'NNP'),\n",
       " ('Home', 'NNP'),\n",
       " ('Care', 'NNP'),\n",
       " ('Omnibus', 'NNP'),\n",
       " (',', ','),\n",
       " ('better', 'RBR'),\n",
       " ('selling', 'VBG'),\n",
       " ('than', 'IN'),\n",
       " ('Fifty', 'NNP'),\n",
       " ('More', 'NNP'),\n",
       " ('Things', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('do', 'VB'),\n",
       " ('in', 'IN'),\n",
       " ('Zero', 'NNP'),\n",
       " ('Gravity', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('more', 'RBR'),\n",
       " ('controversial', 'JJ'),\n",
       " ('than', 'IN'),\n",
       " ('Oolon', 'NNP'),\n",
       " ('Colluphid', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('trilogy', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('philosophical', 'JJ'),\n",
       " ('blockbusters', 'NNS'),\n",
       " ('Where', 'WRB'),\n",
       " ('God', 'NNP'),\n",
       " ('Went', 'NNP'),\n",
       " ('Wrong', 'NNP'),\n",
       " (',', ','),\n",
       " ('Some', 'DT'),\n",
       " ('More', 'JJR'),\n",
       " ('of', 'IN'),\n",
       " ('God', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('Greatest', 'NNP'),\n",
       " ('Mistakes', 'NNPS'),\n",
       " ('and', 'CC'),\n",
       " ('Who', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('this', 'DT'),\n",
       " ('God', 'NNP'),\n",
       " ('Person', 'NNP'),\n",
       " ('Anyway', 'NNP'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(text[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it be also the story of a book , a book call the hitch hiker 's guide to the galaxy - not an earth book , never publish on earth , and until the terrible catastrophe occur , never see or hear of by any earthman .\n"
     ]
    }
   ],
   "source": [
    "# now REALLY put it all together\n",
    "newtext = []\n",
    "wordtype = set(['R','V','N'])\n",
    "\n",
    "for string in text:\n",
    "    newlem = []\n",
    "\n",
    "    taggedlist = nltk.pos_tag(word_tokenize(string.lower()))\n",
    "    for item in taggedlist:\n",
    "        if item[1][0] in wordtype:\n",
    "            postag = item[1][0].lower()\n",
    "        elif item[1][0] == 'J':\n",
    "            postag = 'a'\n",
    "        else:\n",
    "            postag = \"n\"\n",
    "\n",
    "        lemmed = wnl.lemmatize(item[0], pos = postag)\n",
    "        newlem.append(lemmed)\n",
    "    \n",
    "    newstring = \" \".join(newlem)\n",
    "    \n",
    "   \n",
    "\n",
    "    newtext =newtext + [newstring]\n",
    "    \n",
    "print(newtext[9])     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 173)\n",
      "['amazingly', 'ape', 'apocryphal', 'arm', 'backwater', 'bad', 'begin', 'big', 'blockbuster', 'blue', 'book', 'cafe', 'care', 'catastrophe', 'celestial', 'change', 'cheap', 'civilization', 'colluphid', 'come', 'concern', 'consequence', 'contains', 'controversial', 'cover', 'descended', 'digital', 'distance', 'earth', 'earthman', 'eastern', 'encyclopedia', 'end', 'extraordinary', 'fact', 'far', 'finally', 'forever', 'form', 'friendly', 'galactica', 'galaxy', 'girl', 'god', 'good', 'gravity', 'great', 'green', 'guide', 'happy', 'hear', 'highly', 'hiker', 'hitch', 'home', 'house', 'idea', 'important', 'inaccurate', 'increasingly', 'inextricably', 'inscribe', 'insignificant', 'intertwine', 'know', 'knowledge', 'large', 'largely', 'leave', 'letter', 'lie', 'life', 'little', 'lose', 'lot', 'make', 'man', 'mean', 'mile', 'million', 'minor', 'miserable', 'mistake', 'movement', 'nail', 'nearly', 'neat', 'nice', 'ninety', 'occur', 'ocean', 'odd', 'old', 'omission', 'omnibus', 'oolon', 'opinion', 'orbit', 'outer', 'panic', 'paper', 'pedestrian', 'people', 'person', 'philosophical', 'phone', 'piece', 'place', 'planet', 'popular', 'pretty', 'primitive', 'probably', 'problem', 'publish', 'publishing', 'realize', 'relaxed', 'remain', 'remarkable', 'repository', 'respect', 'rickmansworth', 'right', 'rim', 'roughly', 'sadly', 'say', 'score', 'secondly', 'sell', 'simply', 'sit', 'slightly', 'small', 'solution', 'spiral', 'standard', 'story', 'stupid', 'successful', 'suddenly', 'suggest', 'sun', 'supplant', 'tell', 'terrible', 'terribly', 'thing', 'think', 'thousand', 'thursday', 'time', 'tree', 'trilogy', 'uncharted', 'unfashionable', 'unhappy', 'unregarded', 'ursa', 'utterly', 'watch', 'western', 'wholly', 'wildly', 'wisdom', 'word', 'work', 'world', 'wrong', 'year', 'yellow', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# now let's actually create the feature space of interest\n",
    "CV = CountVectorizer(binary=False, stop_words = \"english\")\n",
    "cvtext_dm = CV.fit_transform(newtext)\n",
    "print(cvtext_dm.shape)\n",
    "print(CV.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that the lemmatizer doesn't work well with adverbs.  There's lots of stuff written on this and on how WordNet treats such things.  It involves \"pertainyms\" and other NLP stuff that we don't need to delve into. If adverb lemmatization is important for your question, let me know and I'll share some ugly code that might help\n",
    "\n",
    "\n",
    "#### If you are interested in more detail on POS, see this: http://www.inf.ed.ac.uk/teaching/courses/inf2a/slides/2009_inf2a_L12_slides.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
