{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 - Measuring Similarity, clustering, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### more suggestions for data\n",
    "https://snap.stanford.edu/data/web-Amazon.html - thanks to Leo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of classification\n",
    "\n",
    "Last week we talked about Naive Bayes, Decision Trees, and logistic regression as ways to do classification/prediction with our feature spaces as inputs.  \n",
    "\n",
    "Why not KNN or K-means?  We can certainly use them but not before we discuss distance measures for vectorized text.  How do you know how \"close\" one neighbor is to another?  How do you know how \"far\" one observation is from the centroid?\n",
    "\n",
    "We can turn to our old standby Euclidean distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "pathname = \"C:\\\\Users\\\\Owner\\\\Documents\\\\MS in BIA\\\\Teaching\\\\2018 Fall\\\\Text Mining\\\\\" #where to get/put files\n",
    "pd.set_option('display.max_colwidth', 15000) #important for getting all the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: the **Euclidean distance** calculation can be defined as:\n",
    "\n",
    "\\begin{equation} d = \\sqrt{(X_1 - Y_1)^2 + (X_2 - Y_2)^2 + (X_3 - Y_3)^2 + ... (X_d - Y_d)}\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "# visual representation of distance\n",
    "coords1 = [1, 2, 3]\n",
    "coords2 = [4, 5, 6]\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter((coords1[0], coords2[0]), \n",
    "        (coords1[1], coords2[1]),\n",
    "        (coords1[2], coords2[2]),\n",
    "         color=\"k\", s=150)\n",
    "\n",
    "ax.plot((coords1[0], coords2[0]), \n",
    "        (coords1[1], coords2[1]),\n",
    "        (coords1[2], coords2[2]),\n",
    "         color=\"r\")\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax.text(x=2.5, y=3.5, z=4.0, s=' ')\n",
    "\n",
    "plt.title('Euclidean distance between 2 3D-coordinates') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take an easier example\n",
    "# in one line of code\n",
    "from math import sqrt\n",
    "\n",
    "one = [1,1,1]\n",
    "two = [1,1,3]\n",
    "\n",
    "\n",
    "print(sqrt(sum((x-y)**2 for x,y in zip(one, two))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and more explicitly\n",
    "to_sum = []\n",
    "i = 0\n",
    "for x,y in zip(one, two):\n",
    "    i += 1\n",
    "    print('step 1.' + str(i) + ': print x and y')\n",
    "    print( str(x) + ', ' + str(y))\n",
    "    print( 'step 2.' + str(i) + ': subtract y from x and square')\n",
    "    print( (x-y)**2)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    to_sum.append((x-y)**2)\n",
    "\n",
    "print('step 3: sum squares')\n",
    "print(sum(to_sum))\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print('step 4: square root of sum')\n",
    "print(sqrt(sum(to_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = [one, two] #newest version requires matrix not two lists\n",
    "\n",
    "euclidean_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more detailed example\n",
    "# diagonal elements are always zero because that's the distance of the item to itself\n",
    "\n",
    "A = [[1,0,0,1,0],\n",
    "     [1,0,0,1,0],\n",
    "     [1,0,0,0,0],\n",
    "     [1,1,1,0,1]]\n",
    "\n",
    "print(euclidean_distances(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another important concept: similarity\n",
    "\n",
    "How close or far one vector is from another can be expressed another way: how similar is one vector from another?\n",
    "\n",
    "This actually makes more sense for our text examples.  How \"close\" one sentence (or string) is to another is stranger than how \"similar\" that sentence (or string) is to another.\n",
    "\n",
    "Introducing **cosine similarity**\n",
    "\n",
    "An example: https://www.federalreserve.gov/econresdata/notes/feds-notes/2015/semantic-analysis-of-the-FOMCs-postmeeting-statement-20150930.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cosine similarity** measure for two vectors is defined as:\n",
    "\n",
    "$$ cos(x,y) = \\frac{a \\cdot b}{||a|| ||b||} $$\n",
    "\n",
    "Link: http://blog.christianperone.com/?p=2497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(cosine_similarity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_manual(v1,v2):\n",
    "    # compute cosine similarity of v1 to v2: (v1 dot v1)/(||v1||*||v2||)'\n",
    "    # the L2-normalized dot product of vectors\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        print(\"For element \" + str(i))\n",
    "        print('step 1:  print x and y')\n",
    "        x = v1[i]\n",
    "        y = v2[i]\n",
    "        print(str(x) + ', ' + str(y))\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print('step 2: sum products x,y,xy')\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "        print(\"sum x squared: \" + str(sumxx))\n",
    "        print(\"sum y squared: \" + str(sumyy))\n",
    "        print(\"sum x*y: \" + str(sumxy))\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('step 3: divide sumxy by square root of product sum x, sum y:')\n",
    "    print(str(sumxy) + \" divided by sqrt(\"+str(sumxx)+\"*\"+str(sumyy)+\")\" )\n",
    "    print(str(sumxy)+ \"/\" + str(sqrt(sumxx*sumyy)))\n",
    "    return(sumxy/sqrt(sumxx*sumyy))\n",
    "\n",
    "\n",
    "cosine_similarity_manual(one, two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1,0,0,1,0],\n",
    "     [1,0,0,1,0],\n",
    "     [1,0,0,0,0],\n",
    "     [1,1,1,0,1]]\n",
    "\n",
    "print(cosine_similarity(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember: \n",
    "\n",
    "#### Euclidean distance: more alike vectors are closer together so smaller distances mean more alike.  Smaller numbers mean better fit.  Range from 0 to ? \n",
    "\n",
    "#### Cosine similarity:  if vectors are alike, they have more similarity therefore the similarity measure is a larger number.  Bigger numbers mean better fit.  Range from -1 to 1\n",
    "\n",
    "##### Wait, Negative?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(url= \"http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stupidly simple example (aka why you care about normalization, weights, and vectorizer choice)\n",
    "\n",
    "I can only draw in two dimensions so let's work with text snippets\n",
    "(inspired by http://www.derivativesinvesting.net/article/7072111745/simple-text-retrieval-vector-space-model-explanation/)\n",
    "\n",
    "* D1 =  \"it is hard to determine\"\n",
    "* D2 =  \"this drive has 100GB of memory. make sure the hard drive is fully installed\" \n",
    "\n",
    "Let's draw this in \"hard/drive\" space.\n",
    "* D1:  hard: 1, drive: 0\n",
    "* D2:  hard: 1, drive: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = [1,0]\n",
    "d2 = [1,2]\n",
    "D = [d1, d2]\n",
    "print(euclidean_distances(D))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~\")\n",
    "print(cosine_similarity(D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add a third snippet\n",
    "D1 =  \"it is hard to determine\"\n",
    "D2 =  \"this drive has 100GB of memory. make sure the hard drive is fully installed\"\n",
    "D3 = \"as part of the factory acceptance, every unit gets a hard drive test.\"\n",
    "\n",
    "d3 = [1,1]\n",
    "\n",
    "newD = [d1, d2, d3]\n",
    "\n",
    "# Which document is D3 more similar to?\n",
    "print(euclidean_distances(newD))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~\")\n",
    "print(cosine_similarity(newD))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean distance would say that D1 & D2 are equally similar to D3\n",
    "\n",
    "#### Cosine similarity would say that D2 and D3 are more similar \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(url= \"http://images.slideplayer.com/27/8959772/slides/slide_32.jpg\"))\n",
    "\n",
    "display(Image(url= \"http://nlp.stanford.edu/IR-book/html/htmledition/img411.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does our vectorizer choice affect the measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDexample = [D1, D2, D3]\n",
    "print(HDexample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just like CountVectorizer\n",
    "cvdrive = TfidfVectorizer(binary=False, stop_words='english', use_idf = False, norm = None) \n",
    "cv_drive = cvdrive.fit_transform(HDexample)\n",
    "print(type(cv_drive))\n",
    "print(cv_drive.shape)\n",
    "print(cvdrive.get_feature_names())\n",
    "print(cv_drive.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handy way to compare things in one line of code\n",
    "\n",
    "#We want to use the first document as our reference point and see how the other documents compare\n",
    "#We could use the matrix but this might be easier to read\n",
    "\n",
    "print(\"Euclidean distances:\")\n",
    "print(euclidean_distances(cv_drive[0:1], cv_drive))\n",
    "print(\"Cosine similarities:\")\n",
    "print(cosine_similarity(cv_drive[0:1], cv_drive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we use weights? \n",
    "cvdrivetf = TfidfVectorizer(stop_words='english', norm = None) \n",
    "cv_drivetf = cvdrivetf.fit_transform(HDexample)\n",
    "print(type(cv_drivetf))\n",
    "print(cv_drivetf.shape)\n",
    "print(cvdrivetf.get_feature_names())\n",
    "print(cv_drivetf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean distances:\")\n",
    "print(euclidean_distances(cv_drivetf[0:1], cv_drivetf))\n",
    "print(\"Cosine similarities:\")\n",
    "print(cosine_similarity(cv_drivetf[0:1], cv_drivetf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf weights might be important for other reasons but they don't affect the similarity calculations. (Note: foreshadowing)\n",
    "\n",
    "#### We can use normalization to get euclidean distance and cosine similarity to agree\n",
    "\n",
    "L1 normalization: least absolute differences\n",
    "\n",
    "L2 normalization: least squares - this is the default for tfidf vectorizer\n",
    "\n",
    "http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/\n",
    "\n",
    "Normalization matters: Euclidean distance doesn't take into account vector length, cosine similarity does\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try l1 normalization first \n",
    "cvdrivetf1 = TfidfVectorizer(use_idf = False, stop_words='english', norm = \"l1\") \n",
    "cv_drivetf1 = cvdrivetf1.fit_transform(HDexample)\n",
    "print(type(cv_drivetf1))\n",
    "print(cv_drivetf1.shape)\n",
    "print(cvdrivetf1.get_feature_names())\n",
    "print(cv_drivetf1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean distances:\")\n",
    "print(euclidean_distances(cv_drivetf1[0:1], cv_drivetf1))\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "print(cosine_similarity(cv_drivetf1[0:1], cv_drivetf1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try l2 normalization next\n",
    "cvdrivetf2 = TfidfVectorizer(use_idf = False, stop_words='english', norm = \"l2\") \n",
    "cv_drivetf2 = cvdrivetf2.fit_transform(HDexample)\n",
    "print(type(cv_drivetf2))\n",
    "print(cv_drivetf2.shape)\n",
    "print(cvdrivetf2.get_feature_names())\n",
    "print(cv_drivetf2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean distances:\")\n",
    "print(euclidean_distances(cv_drivetf2[0:1], cv_drivetf2))\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "print(cosine_similarity(cv_drivetf2[0:1], cv_drivetf2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should you use which?\n",
    "\n",
    "The key is in how much variability there is in document length.  \n",
    "\n",
    "Cosine similarity is not affected by vector length while Euclidean distance is.  However, normalization can be used to account for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's consider an application where distance measures are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Nearest neighbor classification requires a distance concept in order to determine which neighbors are nearest. \n",
    "\n",
    "You can do it from scratch: http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/\n",
    "\n",
    "\n",
    "KNN and TFIDF: http://www.sciencedirect.com/science/article/pii/S1877705814003750\n",
    "\n",
    "\n",
    "\n",
    "** class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, kwargs)[source] **\n",
    "\n",
    "Arguments:\n",
    "* n_neighbors - number of neighbors to check\n",
    "* weights - weight function used in prediction. \n",
    "* Possible values:\n",
    "    * ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    * ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "    * These are NOT the same as TfIdf weights\n",
    "* metric : distance metric with the default setting = ‘minkowski’. \"Minkowski distance is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance.\" (Wikipedia)\n",
    "* p: power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. \n",
    "\n",
    "Default settings are k=5, weights = uniform, minkowski distance with p = 2 => Euclidean distance\n",
    "    \n",
    "\n",
    "Good reference with both R and Python: https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pictures of uniform versus distance\n",
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(url='http://scikit-learn.org/stable/_images/sphx_glr_plot_classification_001.png'))\n",
    "display(Image(url='http://scikit-learn.org/stable/_images/sphx_glr_plot_classification_002.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open CSV file\n",
    "newsdf = pd.read_csv(pathname + \"nytnews_tagged.csv\", index_col = 0) \n",
    "print(newsdf.shape)\n",
    "\n",
    "newsdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a feel for the distribution\n",
    "newsdf['Audience'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create our feature space - use code from last week\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words(\"english\")\n",
    "my_stopwords = nltk_stopwords + [\"br\", \"said\", \"0\", '000', '10', '100', '11', '12', '13', '14', u'15', u'16', u'17', u'18', u'19', u'20', u'25', u'30', u'40', u'50', u'500', u'60']\n",
    "\n",
    "# instantiate vectorizer\n",
    "\n",
    "tfidf1 = TfidfVectorizer(lowercase=True, \n",
    "                        stop_words= my_stopwords, \n",
    "                        max_df=0.95, \n",
    "                        min_df=0.05,\n",
    "                        ngram_range = (1,2)) \n",
    "\n",
    "\n",
    "# fit and transform text\n",
    "tfidf_dm = tfidf1.fit_transform(newsdf['cleantext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tfidf_dm.toarray()  #remember this is the output from the vectorizer and we are turning it into an array\n",
    "\n",
    "\n",
    "y = newsdf['Audience'].values #this is an array of labels\n",
    "\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# instantiate learning model (k = 3)\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions\n",
    "knn1_expected = y_test\n",
    "knn1_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(model.score(X_test,y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(knn1_expected, knn1_predicted)))\n",
    "print(metrics.classification_report(knn1_expected, knn1_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate learning model (k = 3 and let's use Manhattan distance - does this make sense?)\n",
    "model = KNeighborsClassifier(n_neighbors=3, p = 1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions\n",
    "knn2_expected = y_test\n",
    "knn2_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(model.score(X_test,y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(knn2_expected, knn2_predicted)))\n",
    "print(metrics.classification_report(knn2_expected, knn2_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we change the number of neighbors?\n",
    "\n",
    "# instantiate learning model \n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions\n",
    "knn3_expected = y_test\n",
    "knn3_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(model.score(X_test,y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(knn3_expected, knn3_predicted)))\n",
    "print(metrics.classification_report(knn3_expected, knn3_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# creating odd list of K for KNN\n",
    "myList = list(range(1,35))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "#takes some time, get a soda...\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we change the number of neighbors?\n",
    "\n",
    "# instantiate learning model \n",
    "model = KNeighborsClassifier(n_neighbors=27)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions\n",
    "knn2_expected = y_test\n",
    "knn2_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(model.score(X_test,y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(knn2_expected, knn2_predicted)))\n",
    "print(metrics.classification_report(knn2_expected, knn2_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about unsupervised learning?  \n",
    "\n",
    "\n",
    "## Remember K-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **k-means** algorithm takes a dataset X of N points as input, together with a parameter K specifying how many clusters to create. The output is a set of K cluster centroids and a labeling of X that assigns each of the points in X to a unique cluster. All points within a cluster are closer in distance to their centroid than they are to any other centroid. By default, k-means uses the squared Euclidean distance measure.\n",
    "\n",
    "https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\n",
    "\n",
    "Two things to note:\n",
    "* K-means are very sensitive to feature scaling - IDF weighting helps improve the quality of the clustering by quite a lot. (In case you forgot why: http://stats.stackexchange.com/questions/89809/is-it-important-to-scale-data-before-clustering)\n",
    "* As k-means is optimizing a non-convex objective function, it will likely end up in a local optimum. Several runs with independent random init might be necessary to get a good convergence.\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In motion: It takes 7 iterations to converge on 7 centroids with 100 observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://datasciencelab.files.wordpress.com/2013/12/p_n100_k7.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "print(\"In motion: It takes 7 iterations to converge on 7 centroids with 100 observations\")\n",
    "display(Image(url='https://datasciencelab.files.wordpress.com/2013/12/p_n100_k7.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In motion: It takes more iterations to converge on 3 centroids with 200 observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://datasciencelab.files.wordpress.com/2013/12/p_n200_k3.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"In motion: It takes more iterations to converge on 3 centroids with 200 observations\")\n",
    "display(Image(url='https://datasciencelab.files.wordpress.com/2013/12/p_n200_k3.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with documents \n",
    "\n",
    "Great example with clustering on top 100 IMDB movies: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning is super fun', 'Python is super, super cool', 'Statistics is cool, too', 'Fun? Data science is more than fun', 'Python is great for machine learning', 'I like football', 'Football is great to watch']\n",
      "Initialization complete\n",
      "Iteration  0, inertia 4.735\n",
      "Iteration  1, inertia 2.910\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 5.850332e-06\n",
      "[1, 1, 2, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "tfidf_simple = TfidfVectorizer(lowercase=True, \n",
    "                        stop_words= 'english', \n",
    "                        ) \n",
    "\n",
    "# let's start with a simple example for clustering:  our group tweets again\n",
    "\n",
    "friend1 = \"Machine learning is super fun\"\n",
    "friend2 = \"Python is super, super cool\"\n",
    "friend3 = \"Statistics is cool, too\"\n",
    "friend4 = \"Fun? Data science is more than fun\"\n",
    "friend5 = \"Python is great for machine learning\"\n",
    "friend6 = \"I like football\"\n",
    "friend7 = \"Football is great to watch\"\n",
    "textStr = [friend1, friend2, friend3, friend4, friend5, friend6, friend7]\n",
    "print(textStr)\n",
    "\n",
    "# fit and transform text\n",
    "\n",
    "simple_dm = tfidf_simple.fit_transform(textStr)\n",
    "My_k = 3\n",
    "km = KMeans(n_clusters=My_k, init='k-means++', max_iter=100, n_init=1, random_state = 42,\n",
    "                verbose=True)\n",
    "km.fit(simple_dm)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning is super fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python is super, super cool</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistics is cool, too</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fun? Data science is more than fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python is great for machine learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I like football</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Football is great to watch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Text  Cluster\n",
       "0         Machine learning is super fun        1\n",
       "1           Python is super, super cool        1\n",
       "2               Statistics is cool, too        2\n",
       "3    Fun? Data science is more than fun        1\n",
       "4  Python is great for machine learning        1\n",
       "5                       I like football        0\n",
       "6            Football is great to watch        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make a data frame\n",
    "text_df = pd.DataFrame(textStr, columns =[\"Text\"])\n",
    "\n",
    "text_df['Cluster'] = clusters\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_dm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9037cfcff116>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# let's use our vectorized news articles and create clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnews_dm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_dm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mMy_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mkm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMy_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_dm' is not defined"
     ]
    }
   ],
   "source": [
    "# let's use our vectorized news articles and create clusters \n",
    "\n",
    "news_dm = tfidf_dm.toarray()\n",
    "My_k = 3\n",
    "km = KMeans(n_clusters=My_k, init='k-means++', max_iter=100, random_state = 42)\n",
    "news_k = km.fit(news_dm)\n",
    "clusters = km.labels_.tolist()\n",
    "newsdf['clusters'] = clusters\n",
    "print(newsdf['clusters'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newsdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_dm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-eab238e8c4c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# finding an optimal value for k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mk_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mk_means_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_dm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk_range\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcentroids_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkm_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkm_result\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk_means_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-eab238e8c4c3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# finding an optimal value for k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mk_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mk_means_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_dm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk_range\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcentroids_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkm_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkm_result\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk_means_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news_dm' is not defined"
     ]
    }
   ],
   "source": [
    "#how many clusters? this takes some time\n",
    "\n",
    "# finding an optimal value for k\n",
    "k_range = range(1,20)\n",
    "k_means_set = [KMeans(n_clusters=k,init='k-means++', max_iter=100, random_state = 42).fit(news_dm) for k in k_range]\n",
    "centroids_list = [km_result.cluster_centers_ for km_result in k_means_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calc euclidean dist from each point to each cluster center\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "k_euclid = [cdist(news_dm, thing, 'euclidean') for thing in centroids_list]\n",
    "distance_set = [np.min(k_euc, axis=1) for k_euc in k_euclid]\n",
    "\n",
    "# total within-cluster sum of squares\n",
    "wcss = [np.sum(distance**2) for distance in distance_set]\n",
    "\n",
    "# total sum of squares\n",
    "tss  = np.sum(pdist(news_dm)**2) / news_dm.shape[0]\n",
    "\n",
    "# between cluster sum of squares\n",
    "bss = tss - wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4802f7e2170a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtss\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'^-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bss' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ceb3c586d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot elbow chart\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(k_range, bss/tss*100, '^-')\n",
    "ax.set_ylim((0,100))\n",
    "plt.grid(True)\n",
    "plt.xlabel('K n_clusters')\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.title('% Var Explained vs K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create clusters - \n",
    "\n",
    "news_dm = tfidf_dm.toarray()\n",
    "My_k = 10\n",
    "km = KMeans(n_clusters=My_k, init='k-means++', max_iter=100, random_state = 42)\n",
    "news_k = km.fit(news_dm)\n",
    "clusters = km.labels_.tolist()\n",
    "newsdf['clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(newsdf['clusters'].value_counts())\n",
    "newsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Main take aways:\n",
    "* We can calculate distance between two vectors in Nspace - this is a measure of similarity of the vector space representation of the text.  \n",
    "* Normalization might be necessary to get the correct distance measure. \n",
    "* KNN is a classification method based on distance and can be used for prediction. \n",
    "* Kmeans clustering is an unsupervised learning method based on distance.\n",
    "* Supervised versus unsupervised learning is another choice that the data scientist needs to make(on top of which classifier to use with which settings if doing supervised learning, label creation options (dictionary/index), preprocesing options (stop word lists, stemming/lemmatization), vectorizer options, parameter settings, etc.))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
